---
title: "StochOptim: A R Wrapper for stocastic Optimisation"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Test 1
Prepare R code for three of the unconstrained test functions in [GO Test Problems](http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar_files/TestGO.htm) that allow dimension greater than 4

* **Rosenbrock Function**
```{r}
Rosenbrock<-function(x){
  len<-length(x)
  sum((100*(x[-len]^2-x[-1])^2) + (x[-len]-rep(1, len-1))^2)
}

#gradient function for Rosenbrock function
Rosenbrock.g<-function(x){
  len<-length(x)
  g<-rep(NA, len)
  g[1]<- 2*(x[1]-1) + 400*x[1]*(x[1]^2-x[2])
  g[2:(len-1)] <- 2*(x[2:(len-1)]-1) + 400*x[2:(len-1)]*(x[2:(len-1)]^2-x[2:(len-1) + 1]) + 200*(x[2:(len-1)]-x[2:(len-1) - 1]^2)
  g[len] <- 200 * (x[len] - x[len - 1]^2)
  g
}
Rosenbrock(c(1,2,3,4))
```
* **Sphere Function**
```{r}
sphere<-function(x){
  sp<-sum(x^2)
  if(is.na(sp)){
    sp<-Inf
  } 
  sp
}

#gradient function for sphere function
sphere.g<-function(x){
  len<-length(x)
  g<-rep(NA, len)
  g[1:len]<-2*x
  g
}

sphere(c(1,2,3,4))
```
* **Sum Squares Function**
```{r}
sum_sq<-function(x){
  len<-length(x)
  ans <- sum(seq(1, len, length=len)*(x^2))
  if(is.na(ans)) ans <- Inf
  ans
}

#gradient function for sum square function
sum_sq.g<-function(x){
  len<-length(x)
  g<-rep(NA, len)
  g[1:len]<-seq(1, len, length=len)*2*x
  g
}

sum_sq(c(1,2,3,4))
```
* **Dixon&Price Function**
```{r}
d_and_p<-function(x){
  len<-length(x)
  t1<-(x[1]-1)^2
  t2<-sum(seq(2, len, length=len-1)*(2*x[-1]^2-x[-len])^2)
  t1+t2
}

#gradient function for dixon&price function
d_and_p.g<-function(x){
  len<-length(x)
  g<-rep(NA, len)
  g[1]<-2*(x[1]-1) - 4*(2*x[2]^2-x[1])
  g[2:(len-1)]<- 8*seq(2,len-1, length=len-2)*x[2:(len-1)]*(2*x[2:(len-1)]^2-x[1:(len-2)]) - 2*seq(3,len, length=len-2)*(2*x[3:len]^2-x[2:(len-1)])
  g[len]<-8*len*x[len]*(2*x[len]^2-x[len-1])
  g
}
d_and_p(c(1,2,3,4))

```


## Test 2
Try to minimize these with the base R **optim()** function. Be sure to document what you do.

```{r}
#Rosenbrock
optm_1<-optim(c(1,2,3,4), Rosenbrock, Rosenbrock.g, method="BFGS")
optm_1

#sphere
optm_2<-optim(c(1,2,3,4), sphere, sphere.g, method="CG")
optm_2

#Sum square
optm_3<-optim(c(1,2,3,4), sum_sq, method="SANN")
optm_3

#dixon&price
optm_4<-optim(c(1,2,3,4), d_and_p, method="Nelder-Mead")
optm_4
```


## Test 3
Run several (at least 4) solvers at once with the **opm()** function
```{r}
require(optimr)
#Rosenbrock
result1<-opm(c(1,2,3,4), Rosenbrock, Rosenbrock.g, method="ALL", control=list(kkt=FALSE, trace=0))
result1

#sphere
result2<-opm(c(1,2,3,4), sphere, sphere.g, method="ALL", control=list(kkt=FALSE, trace=0))
result2

#sum_square
result3<-opm(c(1,2,3,4), sum_sq, sum_sq.g, method="ALL", control=list(kkt=FALSE, trace=0))
result3

#dixon&price
result4<-opm(c(1,2,3,4), d_and_p, d_and_p.g, method="ALL", control=list(kkt=FALSE, trace=0))
result4
```

#### Observations from above Result
* For problems which has quite significant scaling factor(Eg: Rosenbrock function), non-gradient methods such as **Nelder-Mead** and **hjn** does large function evaluations to arrive at results.

* For simpler problems such as Sphere function, **Nelder-Mead** and **hjn** requires relatively less number of evaluations as compared to Rosenbrock problem but still, the number of evaluations involved is larger than compared to gradient optimization methods for the same problem.

* **CG** method reaches its iteration limit and does not able to find the minimum for the Rosenbrock function. Also it can be seen that this method takes relatively more function and gradient computations compared to other gradient methods.

* **Rvmmin** method achieves better results compared to other methods and for the Rosenbrock problem **Rvmmin** comes up with the best accuracy.

* **BFGS**, **L-BFGS-B** and **Rcgmin** are obtaining good results with less number of computations involved.

* **nlminb** sometimes provide **NA/NaN** value as parameter "x" to the function(**Sphere**, **Sum Square**) during the optimization process. Also it should be noted that nlminb is obtaining much closer values of parameters to the exact solution as compared to the parameter values obtained by other solvers.

#### Conlusion

* For the purpose of achieving better results we can use **L-BFGS-B**, **Rvmmin**,**nlminb**, **BFGS** and **Rcgmin** by providing analytic gradient to these methods. It could happen that **nlminb** might provide **NA** values as parameters to the objective function but this could be handled separately by defining our objective functon such that it output Inf in place of NA.
* As **Nelder-Mead** and **hjn** do quite large number of function evaluations to arrive at results so we should not use them for optimizing difficult problems because they might hit the limit of function evaluations in such case.

## Test 4

Choose at least three stochastic optimization solvers from the suggestions in Global and Stochastic Optimization section of the [CRAN Task View: Optimization and Mathematical Programming](https://cran.r-project.org/web/views/Optimization.html) and apply these to your test problems. 

---

### 1. DEoptim

```{r}
require(DEoptim)
```

#### Optimising above function using DEoptim package.
* **Rosenbrock Function**
```{r}
#Rosenbrock
set.seed(1231)
DEoptim_1 <- DEoptim(Rosenbrock, lower=c(-10,-10,-10), upper=c(10,10, 10), DEoptim.control(NP=40))
#plot of the optimization process for rosenbrock function
plot(DEoptim_1)
```

* **Sphere Function**
```{r}
#sphere
set.seed(1232)
DEoptim_2 <- DEoptim(sphere, lower=c(-10,-10,-10), upper=c(10,10, 10), DEoptim.control(NP=40, itermax=50))
#plot of the optimization process for sphere function
plot(DEoptim_2)
```

* **Sum Square Function**
```{r}
#sum square
set.seed(1233)
DEoptim_3 <- DEoptim(sum_sq, lower=c(-10,-10,-10), upper=c(10,10, 10), DEoptim.control(NP=40, itermax=50))
#plot of the optimization process for sum_square function
plot(DEoptim_3)
```

* **Dixon&Price Function**
```{r}
#dixon&price
set.seed(1234)
DEoptim_4 <- DEoptim(d_and_p, lower=c(-10,-10,-10), upper=c(10,10, 10), DEoptim.control(NP=40, itermax=50))
#plot of the optimization process for dixon&price function
plot(DEoptim_4)
```

#### Why DEoptim ? 

* **DEoptim** performs function evaluations repeatedly to reach at better function/parameter values with each iteration.

* The evalations can be performed at faster rate by running the package in parallel mode.

* Output can be visualised easily using the plot method which makes it easy to read and understand. Also the maximum number of iterations to be performed can be changed according to the problem.

* If the function evaluates to **NA** or **NaN** value for any set of parameter then DEoptim stops iterating further. Although the issue can be resolved by defining our function in such a way that it returns Inf in place of NA.

---

### 2. ABCoptim

```{r}
require(ABCoptim)
```

####Optimising above function using ABCoptim package.
* **Rosenbrock Function**
```{r}
#Rosenbrock
ABCoptim_1 <- abc_optim(c(1,2,3), Rosenbrock, lb=-10, ub=10, criter=50, maxCycle=200)
print(ABCoptim_1)
plot(ABCoptim_1, main="Rosenbrock")
```

* **Sphere Function**
```{r}
#sphere
ABCoptim_2 <- abc_optim(c(1,2,3), sphere, lb=-10, ub=10, criter=50)
plot(ABCoptim_2, main="Sphere")
```

* **Sum Square Function**
```{r}
#sum_square
ABCoptim_3 <- abc_optim(c(1,2,3), sum_sq, lb=-10, ub=10, criter=50)
plot(ABCoptim_3, main="Sum Sqaure")
```

* **Dixon&price Function**
```{r}
#dixon&price
ABCoptim_4 <- abc_optim(c(1,2,3), d_and_p, FoodNumber = 50, lb=-10, ub=10, criter=50)
plot(ABCoptim_4, main="Dixon&Price")
```

#### Why ABCoptim ? 

* **abc_optim** is an implementation of **ABC optimization algorithm**. The optimization can be done within less number of iteration if we increase the FoodNumber( i.e. number of food sources ).

* The package also provides **abc_cpp** method which is a C++ implementation of the algorithm and is much more faster then apc_optim.

* Like DEoptim, ABCoptim also provides plot method which makes it easy to visualize the optimization process.

---

### 3. Rgenoud

```{r}
require(rgenoud)
```

#### Optimising above function using rgenoud package.
* **Rosenbrock Function**
```{r}
#Rosenbrock
genoud_1 <- genoud(fn=Rosenbrock, pop.size=2000, nvars=4, starting.values=matrix(1:4, ncol=4), wait.generation=20, print.level=0 )
genoud_1
```

* **Sphere Function**
```{r}
#sphere
genoud_2 <- genoud(fn=sphere, pop.size=2000, nvars=4, starting.values=matrix(1:4, ncol=4), wait.generation=20, print.level=0)
genoud_2
```

* **Sum Square Function**
```{r}
#sum_square
genoud_3 <- genoud(fn=sum_sq, pop.size=2000, nvars=4, starting.values=matrix(1:4, ncol=4), wait.generation=20, print.level=0)
genoud_3
```

* **Dixon&price Function**
```{r}
#dixon&price
genoud_4 <- genoud(fn=d_and_p, pop.size=2000, nvars=4,starting.values=matrix(1:4, ncol=4), wait.generation=20, print.level=0 )
genoud_4
``` 

#### Why rgenoud?

* **rgenoud** provides **genoud** function that works on evolutionary algorithm. The function provides a wide range of arguments to monitor the optimization process. And compared to derivative based optimization methods, **genoud** is much more effective.

* **genoud** function even works for problems for which the derivative information does not exist. 

* The function allows users to set arguments such as **pop.size**(i.e initial population size that the function will use for implementing evolutionary algorithm), **max.generation** and **wait.generation**(i.e limit on the number of generation that would take place while execution of evolutionary algorithm) so that optimization results in more accurate solution.

---

### 4. Genalg

```{r}
#loading package genalg
require(genalg)
```

#### Optimising Rosenbrock function using genalg package.
```{r}

#function to display the progress of the optimisation process.
monitor <- function(obj) {
     # plot the population
     xlim = c(obj$stringMin[1], obj$stringMax[1]);
     ylim = c(obj$stringMin[2], obj$stringMax[2]);
     iteration_num = obj$iter
     lbl = paste("Iteration Number: ", obj$iter);
     if(((iteration_num)%%10)==1 || iteration_num==100){
       plot(obj$population, xlim=xlim, ylim=ylim,
          xlab="x", ylab="y", main=lbl);
     }
}

#Optimising Rosenbrock Function 
rbga.results1 = rbga(c(-10, -10), c(10, 10), monitorFunc=monitor, evalFunc=Rosenbrock, mutationChance=0.001)
```

#### Why genalg?

* **genalg** is an optimization package which is based on genetic algorithm. It provides **rbga** function for the optimization purpose. We can set different parameters such as **mutationChance**, **popSize**, **suggestions** and **iters** according to the need of the problem to be optimised.

* **genalg** package has a **plot** function also which can be used to visualize the optimization process. 

* In addition to the above specified parameters, **rbga** also allows users to monitor the optimization process as it progresses by providing a monitor function to the parameter **monitorFunc**. As in the above R code I have provided a monitor function which plots the population after every (10*x + 1)th iteration of the optimization. 

* Also it can be seen from above plots that optimization using rbga function is quite effective. That is the optimized populations were obtained within less number of iterations and are accurate also.

---

## Task5

Prepare R code for the volcano function of two parameters b=c(x,y) that is defined as f(d) = (10 - 0.5(d)) + sin(2*d) where d is the square norm distance from x=1, y=5.
What are the likely issues in minimizing this function over the two dimensions? A [3D] perspective plot may help you. Do solvers have trouble getting sensible results for this function?

```{r}
volcano<-function(x){
  x <- matrix(x, ncol=2)
  d<-apply(x,1,function(y){ (y[1]-1)^2 + (y[2]-5)^2 })
  ans<-((10 - 0.5*d) + sin(2*d))
  ans
}

x <- seq(-50, 50, length = 101)
y <- seq(-50, 50, length = 101)
X <- as.matrix(expand.grid(x,y))
colnames(X) <- c("x", "y")
# evaluate function
z <- volcano(X)

df <- data.frame(X, z)
# plot the function
library(lattice)
wireframe(z ~ x * y , data=df, main = "Plot of volcano function", shade = TRUE, scales = list(arrows = FALSE), screen = list(z = -50, x = -70))

#using opm solvers for volcano function
result1<-opm(c(0,0), volcano, method=c("L-BFGS-B", "Nelder-Mead", "CG", "Rcgmin", "Rvmmin","BFGS"), control=list(kkt=FALSE, trace=0))
result1
```

#### Issues while optimizing Volcano function.

* The plot indicates that the function acquires its minimum value at infinite. Optimization of such functions might give wrong solutions as it is giving with different **opm** solvers( although the convergence code is 0 ). Also most of the optimization solvers in R requires boundaries to be defined for the parameters, and for the volcano function choosing a boundary is not much convenient.

* Some of the solvers(**ABCoptim**) tend to assign **NA/NaN** value to the parameter "x"(passed to volcano function) during the optimization process which results in **Warning: NA/NaN function evaluation** and they keep on producing the same warning until the iteration limit is reached and in the end they provide a wrong solution.